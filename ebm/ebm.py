import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.nn.utils import clip_grad_norm_, clip_grad_value_
import wandb
from utility import plot_energy, plot_energyND
from sklearn.decomposition import PCA
from torch.autograd import grad

def buffer_training(model, data_loader, optimizer, num_epochs, K, buffer_size, alpha):
    """
    Training loop for energy-based models using a buffer for negative sample generation.

    Args:
        model (nn.Module): The energy-based model to be trained.
        data_loader (DataLoader): Dataloader for positive samples.
        optimizer (Optimizer): Optimizer for training the model.
        num_epochs (int): Number of epochs to train the model.
        K (int): Number of Langevin dynamics steps for negative sample generation.
        buffer_size (int): Maximum size of the buffer storing negative samples.
        alpha (float): Weight for L2 regularization term in the loss function.

    This function implements a buffer-based training loop that alternates between
    generating negative samples using Langevin dynamics and updating the model 
    parameters using an energy-based loss function. A buffer is maintained to store
    and reuse negative samples for efficient sampling.
    """

    buffer = None  # Initialize buffer as None, to be created after the first batch

    for step in range(num_epochs):
        for x_pos in data_loader:
            # Initialize buffer with the correct shape after receiving the first batch
            if buffer is None:
                buffer = torch.empty((0, *x_pos.shape[1:]))  # Initialize an empty buffer with the correct shape

            # Sampling negative examples from buffer or uniform distribution
            if buffer.size(0) > 0 and torch.rand(1).item() < 0.90:
                indices = torch.randint(0, buffer.size(0), (x_pos.size(0),))  # Generate indices for the batch size
                x_neg = buffer[indices]  # Index the buffer tensor using the generated indices
            else:
                x_neg = torch.rand_like(x_pos)  # Uniform random samples if buffer is not used

            # Perform Langevin dynamics to generate negative samples
            x_neg = langevin_dynamics(model, x_neg, num_steps=K, step_size=1, noise_scale=0.01)

            # Calculate energy for positive and negative samples
            energy_pos = model(x_pos)
            energy_neg = model(x_neg)

            # Compute the loss: Î± * L2 + LML
            l2_loss = (energy_pos ** 2 + energy_neg ** 2).mean()
            ml_loss = (energy_pos - energy_neg).mean()
            loss = alpha * l2_loss + ml_loss

            # Update model parameters
            optimizer.zero_grad()
            loss.backward()

            # Apply gradient clipping to stabilize training
            clip_grad_norm_(model.parameters(), 1)
            optimizer.step()

            # Detach x_neg from the computation graph and add it to the buffer
            x_neg = x_neg.detach()

            if buffer.size(0) == 0:
                buffer = x_neg
            else:
                buffer = torch.cat((buffer, x_neg), dim=0)  # Concatenate the new samples to the buffer

            # If the buffer exceeds the buffer_size, remove the oldest entries
            if buffer.size(0) > buffer_size:
                # Add x_pos to the buffer
                buffer = buffer[-buffer_size:]

        print(f'Step [{step + 1}/{num_epochs}], Loss: {loss.item()}')

def train(model, dataloader, optimizer, num_epochs=100, num_steps=10, step_size=0.1, noise_scale=0.01, log=False, device='cpu'):
    """
    Train the energy-based model using stochastic gradient descent and Langevin dynamics.

    Args:
        model (nn.Module): The energy-based model to be trained.
        dataloader (DataLoader): Dataloader providing batches of training samples.
        optimizer (Optimizer): Optimizer for the training process.
        num_epochs (int): Number of epochs for training.
        num_steps (int): Number of Langevin steps for negative sample generation.
        step_size (float): Step size for Langevin dynamics.
        noise_scale (float): Standard deviation of the noise added during Langevin dynamics.
        log (bool): Whether to log metrics and plots to WandB.
        device (str): Device to use for training ('cpu' or 'cuda').

    Implements a training loop where, for each epoch, the model is updated using both positive
    samples (from data) and negative samples generated by Langevin dynamics. Regular logging 
    and plotting (if enabled) help monitor the training progress.
    """
    model.to(device)
    model.train()

    best_loss = float('inf')  # Start with a high initial loss for early stopping
    no_improve_epochs = 0     # Counter for epochs with no improvement

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        log_pos, epoch_plot_energy = [], False
        for i, data in enumerate(dataloader):
            if(epoch % 10 == 0 and epoch > 0):
                log_pos.append(data)
                epoch_plot_energy = True
            x_data = data.to(device)
            pos_energy = model(x_data).mean()
    
            # With probability 90% use x_data, with probability 10% start from random points
            mask = torch.rand(x_data.shape[0], 1) < 0.9
            x_sample = torch.where(mask, x_data, torch.randn_like(x_data))

            # Negative phase: Langevin dynamics
            x_sample = langevin_dynamics(model, x_sample, num_steps, step_size, noise_scale)
            neg_energy = model(x_sample).mean()

            # Loss: Mean squared difference between energy of data and samples
            loss = pos_energy - neg_energy

            # Backpropagation and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

            if i % 10 == 0:
                print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.12f}")
        
        if(epoch_plot_energy and log):
            log_pos = torch.cat(log_pos, dim=0).detach().numpy()
            log_neg = langevin_dynamics(model, torch.randn_like(x_data), num_steps, step_size, noise_scale).detach().numpy()
            
            # Perform 2D PCA of log_pos and log_neg
            pca = PCA(n_components=2)
            log_pos = pca.fit_transform(log_pos)
            log_neg = pca.transform(log_neg)
            
            # Normalize between -1 and 1
            log_pos = (log_pos - log_pos.min()) / (log_pos.max() - log_pos.min()) * 2 - 1
            log_neg = (log_neg - log_neg.min()) / (log_neg.max() - log_neg.min()) * 2 - 1

            en_plot = plot_energyND(model, x_interval=(-2, 2), y_interval=(-2, 2), pos=log_pos, neg=log_neg)

            wandb.log({"energy_plot": wandb.Image(en_plot)})

        # Calculate average loss for the epoch
        avg_epoch_loss = epoch_loss / len(dataloader)
        print(f"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_epoch_loss:.12f}")
        if(log):
            wandb.log({
                "epoch": epoch + 1,
                "avg_epoch_loss": avg_epoch_loss
            })

        # Check for improvement
        if np.abs(avg_epoch_loss) < best_loss:
            best_loss = avg_epoch_loss
            no_improve_epochs = 0  # Reset counter if there is improvement
        else:
            no_improve_epochs += 1

    wandb.finish()

def generate_data(model, num_samples=1000, num_steps=1000, step_size=0.1, noise_scale=0.01, device='cpu'):
    """
    Generates synthetic data using a trained energy-based model and Langevin dynamics.

    Args:
        model (nn.Module): The trained energy-based model.
        num_samples (int, optional): Number of samples to generate. Defaults to 1000.
        num_steps (int, optional): Number of Langevin steps for sampling. Defaults to 1000.
        step_size (float, optional): Step size for Langevin dynamics. Defaults to 0.1.
        noise_scale (float, optional): Scale of Gaussian noise added at each step. Defaults to 0.01.
        device (str, optional): Device to perform computations ('cpu' or 'cuda'). Defaults to 'cpu'.

    Returns:
        numpy.ndarray: Array of generated samples.
    """
    model.to(device)
    model.eval()

    # Initialize random samples from a normal distribution
    x_init = torch.randn(num_samples, 2).to(device)
    # Generate samples using Langevin dynamics
    x_samples = langevin_dynamics(model, x_init, num_steps=num_steps, step_size=step_size, noise_scale=noise_scale)

    return x_samples.detach().cpu().numpy()


class Swish(nn.Module):
    """
    Swish activation function.
    """
    def forward(self, x):
        return x * torch.sigmoid(x)


class ConvEnergyModel(nn.Module):
    """
    Convolutional energy-based model for 2D inputs.

    Args:
        dim (int, optional): Input dimension. Defaults to 2.
        n_f (int, optional): Number of filters for the convolutional layers. Defaults to 32.
        leak (float, optional): Negative slope for the LeakyReLU activation. Defaults to 0.05.
    """
    def __init__(self, dim=2, n_f=32, leak=0.05):
        super(ConvEnergyModel, self).__init__()
        self.f = nn.Sequential(
            nn.Conv2d(dim, n_f, 1, 1, 0),
            nn.LeakyReLU(leak),
            nn.Conv2d(n_f, n_f * 2, 1, 1, 0),
            nn.LeakyReLU(leak),
            nn.Conv2d(n_f * 2, n_f * 2, 1, 1, 0),
            nn.LeakyReLU(leak),
            nn.Conv2d(n_f * 2, 1, 1, 1, 0)
        )

    def forward(self, x):
        # Reshape input to (Batch size, 2, 1, 1)
        x = x.view(x.size(0), 2, 1, 1)
        return self.f(x).squeeze()


class ResidualBlock(nn.Module):
    """
    A residual block consisting of two fully connected layers and a skip connection.

    Args:
        in_features (int): Number of input features.
        out_features (int): Number of output features.
    """
    def __init__(self, in_features, out_features):
        super(ResidualBlock, self).__init__()
        self.fc1 = nn.Linear(in_features, out_features)
        self.fc2 = nn.Linear(out_features, out_features)
        self.swish = Swish()
        
        # Ensure the input and output dimensions are the same for the skip connection
        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()

    def forward(self, x):
        identity = self.shortcut(x)  # Skip connection
        out = self.swish(self.fc1(x))
        out = self.fc2(out)
        out += identity  # Add the input (skip connection)
        return self.swish(out)


class ResidualNetwork(nn.Module):
    """
    Residual network with an initial linear layer, a residual block, and a final linear layer.
    """
    def __init__(self):
        super(ResidualNetwork, self).__init__()
        self.fc_initial = nn.Linear(2, 512)  # Initial layer to project 2D input to a higher dimension
        self.res_block1 = ResidualBlock(512, 512)  # First residual block
        self.fc_final = nn.Linear(512, 1)  # Final layer for energy output
        self.swish = Swish()

    def forward(self, x):
        x = self.swish(self.fc_initial(x))
        x = self.res_block1(x)
        x = self.fc_final(x)
        return x  # Return energy


class EnergyBasedModel(nn.Module):
    """
    A fully connected energy-based model with three layers.
    """
    def __init__(self):
        super(EnergyBasedModel, self).__init__()
        self.fc1 = nn.Linear(100, 1000)  # Increased width
        self.fc2 = nn.Linear(1000, 2)
        self.fc3 = nn.Linear(2, 1)
        self.swish = Swish()  # Swish activation (SiLU in PyTorch)

    def forward(self, x):
        x = self.swish(self.fc1(x))
        x = self.swish(self.fc2(x))
        x = self.fc3(x)
        return x

    def project2D(self, x):
        """
        Projects input data to 2D.
        """
        x = self.swish(self.fc1(x))
        x = self.swish(self.fc2(x))
        return x

    def forward2D(self, x):
        """
        Processes a 2D input through the final layer.
        """
        x = self.fc3(x)
        return x


class Sampler:
    """
    Sampler for generating samples using Langevin dynamics with a neural network model.

    Args:
        model (nn.Module): Neural network to use for modeling E_theta.
        input_shape (tuple): Shape of the input to the model.
        num_steps (int): Number of steps for sampling.
    """
    def __init__(self, model, input_shape, num_steps):
        self.model = model
        self.input_shape = input_shape
        self.buffer = [torch.rand_like(input_shape[1]) for _ in range(input_shape[0])]

    def sample_new_exmps(self, steps=100, step_size=1):
        """
        Generates new samples by performing Langevin dynamics.

        Args:
            steps (int, optional): Number of Langevin steps. Defaults to 100.
            step_size (float, optional): Step size for Langevin dynamics. Defaults to 1.

        Returns:
            torch.Tensor: Tensor of generated samples.
        """
        x_sample = torch.stack(self.examples)
        x_sample = langevin_dynamics(self.model, x_sample, num_steps=steps, step_size=step_size)
        return x_sample


def langevin_dynamics(model, x_init, num_steps=10, step_size=0.1, noise_scale=0.01):
    """
    Performs Langevin dynamics to generate samples from an energy-based model.

    Args:
        model (nn.Module): The trained energy-based model.
        x_init (torch.Tensor): Initial tensor for Langevin dynamics.
        num_steps (int, optional): Number of steps for Langevin dynamics. Defaults to 10.
        step_size (float, optional): Step size for Langevin dynamics. Defaults to 0.1.
        noise_scale (float, optional): Scale of Gaussian noise added at each step. Defaults to 0.01.

    Returns:
        torch.Tensor: Generated samples after Langevin dynamics.
    """
    x = x_init.clone().detach()
    x.requires_grad_(True)

    for _ in range(num_steps):
        energy = model(x)
        energy.backward(torch.ones_like(energy))
        # Gradient descent on energy
        with torch.no_grad():
            x -= step_size * x.grad
            # Adding Gaussian noise
            x += noise_scale * torch.randn_like(x)
        x.grad.zero_()

    return x

def compute_gradients(model, x):
    """
    Compute the gradients of the energy function with respect to the inputs.

    Args:
        model (nn.Module): The energy-based model (PyTorch model).
        x (torch.Tensor): Input tensor for which to compute the gradients.

    Returns:
        torch.Tensor: Gradients tensor.
    """
    x.requires_grad_(True)
    energy_scores = model(x)
    # Compute gradients of energy_scores w.r.t. input x
    gradients = grad(outputs=energy_scores, inputs=x, grad_outputs=torch.ones_like(energy_scores), create_graph=True)[0]
    return gradients


def contrastive_divergence(model, x_data, num_steps=40, step_size=0.1, noise_scale=0.1):
    """
    Perform contrastive divergence to compute the loss for training an energy-based model.

    Args:
        model (nn.Module): The energy-based model.
        x_data (torch.Tensor): Real data samples.
        num_steps (int, optional): Number of Langevin steps for sampling. Defaults to 40.
        step_size (float, optional): Step size for Langevin dynamics. Defaults to 0.1.
        noise_scale (float, optional): Scale of Gaussian noise added during Langevin dynamics. Defaults to 0.1.

    Returns:
        torch.Tensor: The computed contrastive divergence loss.
    """
    # Positive phase: Energy of the data
    energy_data = model(x_data).mean()

    # Negative phase: Initialize with a replay buffer (90% x_data, 10% random points)
    mask = torch.rand(x_data.shape[0], 1) < 0.9
    x_sample = torch.where(mask, x_data, torch.randn_like(x_data))

    # Negative phase: Langevin dynamics to update samples
    x_sample = langevin_dynamics(model, x_sample, num_steps, step_size, noise_scale)
    energy_sample = model(x_sample).mean()

    # Loss: Difference between energies of data and generated samples
    loss = energy_data - energy_sample

    return loss


def effort_functional(model1, model2, init, alpha, beta, step_size=0.1, noise_scale=0.01, num_steps=100):
    """
    Compute the effort functional over a given path using Stochastic Gradient Langevin Dynamics (SGLD).

    Args:
        model1 (nn.Module): The first energy-based model (PyTorch model).
        model2 (nn.Module): The second energy-based model (PyTorch model).
        init (numpy.ndarray): Initial point.
        alpha (float): Weight for the gradient of the first energy model.
        beta (float): Weight for the gradient of the second energy model.
        step_size (float, optional): Step size for the SGLD update. Defaults to 0.1.
        noise_scale (float, optional): Scale of noise added to the SGLD update. Defaults to 0.01.
        num_steps (int, optional): Number of discrete steps for integration. Defaults to 100.

    Returns:
        float: The computed effort functional value.
    """
    total_effort = 0.0
    current_point = torch.tensor(init, dtype=torch.float32)

    for _ in range(num_steps):
        current_point_tensor = current_point.clone().detach().requires_grad_(True)

        # Compute gradients at the current point for both models
        grad1 = compute_gradients(model1, current_point_tensor)
        grad2 = compute_gradients(model2, current_point_tensor)

        # Compute SGLD update
        noise = noise_scale * torch.randn_like(current_point_tensor)
        update = step_size * (alpha * grad1 + beta * grad2) + noise
        next_point = current_point_tensor + update

        # Compute segment length and effort contribution
        segment_length = torch.norm(next_point - current_point_tensor).item()
        norm_grad1 = grad1.norm(p=2).item()
        norm_grad2 = grad2.norm(p=2).item()
        effort = segment_length * (alpha * norm_grad1 + beta * norm_grad2)
        total_effort += effort

        # Update the current point
        current_point = next_point.detach()

    return total_effort


def effort_functional_batch(model1, model2, init_batch, alpha, beta, num_steps=100, step_size=0.01):
    """
    Compute the effort functional over a batch of initial points.

    Args:
        model1 (nn.Module): The first energy-based model (PyTorch model).
        model2 (nn.Module): The second energy-based model (PyTorch model).
        init_batch (torch.Tensor): A batch of initial points (tensor of shape [batch_size, dim]).
        alpha (float): Weight for the gradient of the first energy model.
        beta (float): Weight for the gradient of the second energy model.
        num_steps (int, optional): Number of discrete steps for integration. Defaults to 100.
        step_size (float, optional): Step size for each Langevin step. Defaults to 0.01.

    Returns:
        torch.Tensor: A tensor of shape [batch_size] containing the computed effort functional values for each initial point.
    """
    # Initialize the total effort for each initial point in the batch
    batch_size, dim = init_batch.shape
    total_effort = torch.zeros(batch_size, dtype=torch.float32)

    # Initialize the previous points with the input batch
    prev_points = init_batch.clone().detach()

    for _ in range(num_steps):
        # Compute gradients at current points for both models
        grad1 = compute_gradients(model1, prev_points)
        grad2 = compute_gradients(model2, prev_points)

        # Update current points using a step in the direction of the combined gradients
        noise = torch.randn_like(prev_points)  # Add noise for stochasticity
        current_points = prev_points + alpha * grad1 + beta * grad2 + step_size * noise

        # Compute the norm of gradients
        norm_grad1 = grad1.norm(p=2, dim=1)
        norm_grad2 = grad2.norm(p=2, dim=1)

        # Compute segment lengths and effort contributions
        segment_lengths = torch.norm(current_points - prev_points, dim=1)
        effort = segment_lengths * (alpha * norm_grad1 + beta * norm_grad2)
        total_effort += effort

        # Update previous points
        prev_points = current_points

    return total_effort